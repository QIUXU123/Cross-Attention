{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ffmpeg as ff\n",
    "import os\n",
    "from typing import List, Tuple, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "import pickle\n",
    "import dlib\n",
    "import datetime\n",
    "from facenet_pytorch import MTCNN\n",
    "import logging\n",
    "from einops import rearrange\n",
    "import logging\n",
    "import torch\n",
    "from torchvision import transforms, utils, models\n",
    "from PIL import Image\n",
    "import face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('[%(asctime)s]::%(module)s::%(levelname)s::%(message)s')\n",
    "streamHandler = logging.StreamHandler()\n",
    "streamHandler.setFormatter(formatter)\n",
    "fileHandler = logging.FileHandler('./LOG/personalityLog.log')\n",
    "fileHandler.setFormatter(formatter)\n",
    "logger.addHandler(streamHandler)\n",
    "logger.addHandler(fileHandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
    "margin = 60\n",
    "number_of_samples =15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detector = dlib.get_frontal_face_detector()   \n",
    "landmark_detector = dlib.shape_predictor(\"detector/shape_predictor_68_face_landmarks.dat\")\n",
    "LIP_MARGIN = 0.5                # Marginal rate for lip-only image.\n",
    "RESIZE = (224,224) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE_CUDA = torch.cuda.is_available()\n",
    "# device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
    "mtcnn= MTCNN(image_size=224, margin=margin, min_face_size= 60, thresholds=[0.6, 0.7, 0.7], post_process=True,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_frames(file_path: str) -> int:\n",
    "    probe = ff.probe(file_path)\n",
    "    video_streams = [stream for stream in probe[\"streams\"] if stream[\"codec_type\"] == \"video\"]\n",
    "    #width = video_streams[0]['coded_width']\n",
    "    #height = video_streams[0]['coded_height']\n",
    "    del probe\n",
    "    return video_streams[0]['nb_frames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_from_video(file_path: str) -> np.ndarray:\n",
    "    inputfile = ff.input(file_path)#입력 파일 만들기\n",
    "    out = inputfile.output('-', format='f32le', acodec='pcm_f32le', ac=1, ar='44100')#추출할 오디오 데이터 형식 지정（字节数组）\n",
    "    raw = out.run(capture_stdout=True)#오디오 파일 추출\n",
    "    del inputfile, out\n",
    "    return np.frombuffer(raw[0],np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio_series(raw_data: np.ndarray) -> np.ndarray:\n",
    "    N, M = 24, 1319\n",
    "    mfcc_data = librosa.feature.mfcc(y=raw_data, sr=44100, n_mfcc=24)#计算raw_data的MFCC特征 ,  n_mfcc=24  Mel Frequency Cepstrum Coefficient\n",
    "# sr=44100,\n",
    "    # Getting spectral mean (centroid)\n",
    "    # mean = librosa.feature.spectral_centroid(result)\n",
    "    # Standardizing MFCC (zero mean and unit variance)\n",
    "    mfcc_data_standardized = (mfcc_data - np.mean(mfcc_data)) / np.std(mfcc_data)#mfcc_data를 표준화하다\n",
    "    # Use pre-padding (Note: with 0, which is also the mean after standardization) to unify the length of the samples.\n",
    "    number_of_columns_to_fill = M - mfcc_data_standardized.shape[1]#0을 몇 줄 더 보충해야 합니다.\n",
    "    padding = np.zeros((N, number_of_columns_to_fill))\n",
    "    padded_data = np.hstack((padding, mfcc_data_standardized))#배열이 수평으로 쌓입니다\n",
    "   \n",
    "    return padded_data.reshape(N, M, 1) # Reshaping to N,M,1\n",
    "    #给定的(N, M, 1)形状可以看作是在指定时间内检测到的M个频率的强度值，其中N是MFCC特征数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_to_list(shape):\n",
    "\tcoords = []\n",
    "\tfor i in range(36, 42):\n",
    "\t\tcoords.append((shape.part(i).x, shape.part(i).y))\n",
    "\treturn coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image_to_eyes(frame,landmark):\n",
    "    lip_landmark = landmark[0:68]                                          # Landmark corresponding to lip\n",
    "    lip_x = sorted(lip_landmark,key = lambda pointx: pointx[0])             # Lip landmark sorted for determining lip region\n",
    "    lip_y = sorted(lip_landmark, key = lambda pointy: pointy[1])\n",
    "    x_add = int((-lip_x[0][0]+lip_x[-1][0])*LIP_MARGIN)                     # Determine Margins for lip-only image\n",
    "    y_add = int((-lip_y[0][1]+lip_y[-1][1])*LIP_MARGIN)\n",
    "    crop_pos = (lip_x[0][0]-x_add, lip_x[-1][0]+x_add, lip_y[0][1]-y_add, lip_y[-1][1]+y_add)   # Crop image\n",
    "    cropped = frame[crop_pos[2]:crop_pos[3],crop_pos[0]:crop_pos[1]]\n",
    "    if cropped.size != 0:\n",
    "        cropped = cv2.resize(cropped,(RESIZE[0],RESIZE[1]),interpolation=cv2.INTER_CUBIC)        # Resize\n",
    "        return cropped\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#추출 할수 있는 얼굴\n",
    "def extract_N_video_frames(frame_num: str ,file_path: str, number_of_samples: int = 15) -> List[np.ndarray]:\n",
    "    full_video_frames = []#프레임 데이터를 저장할 배열 만들기\n",
    "    eyes_video_frames = []\n",
    "                   # Final image size\n",
    "    trans=transforms.ToTensor()\n",
    "    \n",
    "    frame_num=number_of_samples #추출된 프레임의 개수\n",
    "    begin_num=0 # 프레임의 시작index\n",
    "    indexes=[None] * number_of_samples # [0,2.7,5.4......]->[0,2,5......] // [0, 1, 2, 3, 4, 5.... 450?]\n",
    "    get_frames=int(get_number_of_frames(file_path))\n",
    "    x=get_frames/number_of_samples\n",
    "    \n",
    "    for i in range(frame_num):\n",
    "        indexes[i]=int(begin_num)\n",
    "        # print(type(get_number_of_frames(file_path)))\n",
    "        begin_num+=x\n",
    "    j=0    \n",
    "    \n",
    "    cap = cv2.VideoCapture(file_path)\n",
    "    logger.debug(file_path)\n",
    "    for ind in indexes:\n",
    "        frame_count = 0\n",
    "        # print(ind,'\\n')\n",
    "        i=ind\n",
    "        first_go_back_to_before_frames =0\n",
    "        cap.set(1, ind)\n",
    "        res, frame = cap.read()#프레임을 읽습니다\n",
    "        # print(\"framedatatype\",type(frame))\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        face = face_detector(frame,1)#1111111111111111111111111111\n",
    "        while len(face)<1:\n",
    "            if frame_count > x:\n",
    "                if first_go_back_to_before_frames==0:\n",
    "                    i = j+1\n",
    "                    frame_count = 0\n",
    "                    first_go_back_to_before_frames=1\n",
    "                elif first_go_back_to_before_frames==1:\n",
    "                    break\n",
    "            i+=1\n",
    "            if i>=get_frames:\n",
    "                i=j+1\n",
    "            cap.set(1,i)\n",
    "            res, frame = cap.read()\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            face = face_detector(frame,1)#1111111111111111111111111111111\n",
    "            frame_count+=1\n",
    "        j=i\n",
    "        if len(face)<1:\n",
    "            logger.debug(\"no dectected face\")\n",
    "            break\n",
    "        elif len(face)>1:\n",
    "            logger.debug(\"too many dectected faces\")\n",
    "            break\n",
    "        else:\n",
    "            rect = face[0]                    # Proper number of face\n",
    "            landmark = landmark_detector(frame, rect)\n",
    "            landmark = shape_to_list(landmark)   # Detect face landmarks\n",
    "            eyes = crop_image_to_eyes(frame,landmark)\n",
    "            if eyes is not None:\n",
    "                eyes = np.uint8(eyes)\n",
    "                eyes_video_frames.append(cv2.cvtColor(cv2.cvtColor(eyes, cv2.COLOR_BGR2RGB), cv2.COLOR_BGR2RGB))  \n",
    "                full_video_frames.append(frame)#RGB 형식으로 변환하여 video_frames에 추가합니다\n",
    "\n",
    "    cap.release()\n",
    "    del cap, indexes\n",
    "    return full_video_frames,eyes_video_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image: np.ndarray, new_size: Tuple[int,int]) -> np.ndarray:\n",
    "    image = np.array(image)\n",
    "    return cv2.resize(image, new_size, interpolation = cv2.INTER_AREA)\n",
    "#在训练模型时，选取图像的一个随机128x128的窗口；在预测模型时，选取图像中心的128x128窗口。最后返回裁剪后的图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image_window(image: np.ndarray, training: bool = True) -> np.ndarray:#这个不是太理解·······························\n",
    "    height, width, _ = image.shape\n",
    "    if training:\n",
    "        MAX_N = height - 128\n",
    "        MAX_M = width - 128\n",
    "        rand_N_index, rand_M_index = random.randint(0, MAX_N) , random.randint(0, MAX_M)\n",
    "        return image[rand_N_index:(rand_N_index+128),rand_M_index:(rand_M_index+128),:]\n",
    "    else:\n",
    "        N_index = (height - 128) // 2\n",
    "        M_index = (width - 128) // 2\n",
    "        return image[N_index:(N_index+128),M_index:(M_index+128),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading_label_data(file_name: str, dictionary: Dict[str,str]) -> np.ndarray:#영상의 5대 점수를 획득했습니다\n",
    "    features = ['extraversion', 'neuroticism', 'agreeableness', 'conscientiousness', 'openness']\n",
    "    extracted_data = [float(dictionary[label][file_name]) for label in features]\n",
    "    return np.stack(extracted_data).reshape(5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_input(file_path: str, file_name: str, dictionary: Dict[str, str], training: bool = True) -> Tuple[\n",
    "    np.ndarray, np.ndarray, np.ndarray]:\n",
    "    # Audio\n",
    "    extracted_audio_raw = extract_audio_from_video(file_path=file_path)#오디오 데이터 가져오기\n",
    "    preprocessed_audio = preprocess_audio_series(raw_data=extracted_audio_raw)#오디오 파일 처리\n",
    "    frame_num = int(get_number_of_frames(file_path))\n",
    "    # Video\n",
    "    full_video_frames,face_video_frames = extract_N_video_frames(frame_num=frame_num,file_path=file_path, number_of_samples=number_of_samples)#128프레임 획득\n",
    "    resized_Full_images = [resize_image(image=im, new_size=(224, 224)) for im in full_video_frames]#크기를 조정하다\n",
    "    # cropped_images = [crop_image_window(image=resi, training=training) / 255.0 for resi in resized_images]#对图像进行裁剪\n",
    "    if len(full_video_frames)==number_of_samples:\n",
    "        preprocessed_full_video = np.stack(resized_Full_images)#하나의 배열을 합성하다\n",
    "        preprocessed_face_video = np.stack(face_video_frames)\n",
    "        # Ground Truth\n",
    "        video_gt = reading_label_data(file_name=file_name, dictionary=dictionary)#태그 읽기\n",
    "        del face_video_frames,full_video_frames\n",
    "        return (preprocessed_audio,preprocessed_full_video,preprocessed_face_video, video_gt)\n",
    "    return (None,None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_to_expected_fullinput(dataset: List[Tuple[ np.ndarray, np.ndarray]]) -> Tuple[\n",
    "     np.ndarray, np.ndarray]:\n",
    "    x0_list = []\n",
    "    x1_list = []\n",
    "    x2_list = []\n",
    "    for i in range(0, len(dataset)):\n",
    "        x0_list.append(dataset[i][0])\n",
    "        x1_list.append(dataset[i][1])\n",
    "        x2_list.append(dataset[i][3])\n",
    "    return (np.stack(x0_list), np.stack(x1_list),np.stack(x2_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_to_expected_faceinput(dataset: List[Tuple[ np.ndarray, np.ndarray]]) -> Tuple[\n",
    "     np.ndarray, np.ndarray]:\n",
    "    x0_list = []\n",
    "    x1_list = []\n",
    "    x2_list = []\n",
    "    for i in range(0, len(dataset)):\n",
    "        x0_list.append(dataset[i][0])\n",
    "        x1_list.append(dataset[i][2])\n",
    "        x2_list.append(dataset[i][3])\n",
    "    return (np.stack(x0_list), np.stack(x1_list),np.stack(x2_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_data = []\n",
    "fullsavename = '/home/ssrlab/qx/video-swin-transformer-pytorch/data/15Frames/eyes/full/valid_set.dat'\n",
    "facesavename = '/home/ssrlab/qx/video-swin-transformer-pytorch/data/15Frames/eyes/eyes/valid_set.dat'\n",
    "path ='/home/ssrlab/qx/Big5/valid'\n",
    "gt = pickle.load( open( \"/home/ssrlab/qx/Big5/gt/annotation_validation.pkl\", \"rb\" ), encoding='latin1' )#태그 정보 얻기\n",
    "t1 = datetime.datetime.utcnow()\n",
    "i=1\n",
    "filenum=1 #500개의 비디오마다 하나의 파일에 저장\n",
    "for filename in os.listdir(path):#파일 내 비디오 둘러보기\n",
    "    filePath = path+'/'+filename\n",
    "    data = preprocessing_input(file_path= filePath, file_name= filename, dictionary= gt, training= True)\n",
    "    if int(get_number_of_frames(filePath))>128:\n",
    "        if data[0] is None:\n",
    "            continue\n",
    "        else: \n",
    "            training_set_data.append(data)\n",
    "            print('2000/',i)\n",
    "            i+=1\n",
    "with open(fullsavename, \"wb\") as f:\n",
    "    pickle.dump(reshape_to_expected_fullinput(training_set_data), f)\n",
    "fullsavename = []\n",
    "with open(facesavename, \"wb\") as f:\n",
    "    pickle.dump(reshape_to_expected_faceinput(training_set_data), f)\n",
    "facesavename = []\n",
    "t2 = datetime.datetime.utcnow()\n",
    "#Measuring execution time\n",
    "print('Elapsed time: ' + str(t2-t1))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_data = []\n",
    "fullsavename = '/home/ssrlab/qx/video-swin-transformer-pytorch/data/15Frames/eyes/full/train_set.dat'\n",
    "facesavename = '/home/ssrlab/qx/video-swin-transformer-pytorch/data/15Frames/eyes/eyes/train_set.dat'\n",
    "path ='/home/ssrlab/qx/Big5/train'\n",
    "gt = pickle.load( open( \"/home/ssrlab/qx/Big5/gt/annotation_training.pkl\", \"rb\" ), encoding='latin1' )#태그 정보 얻기\n",
    "t1 = datetime.datetime.utcnow()\n",
    "\n",
    "i=1\n",
    "filenum=1 #500개의 비디오마다 하나의 파일에 저장\n",
    "for filename in os.listdir(path):#파일 내 비디오 둘러보기\n",
    "    filePath = path+'/'+filename\n",
    "    data = preprocessing_input(file_path= filePath, file_name= filename, dictionary= gt, training= True)\n",
    "    frame_num = int(get_number_of_frames(filePath))\n",
    "    if frame_num>128:\n",
    "        if data[0] is None:\n",
    "            continue\n",
    "        else: \n",
    "            training_set_data.append(data)\n",
    "            print('6000/',i)\n",
    "            i+=1\n",
    "with open(fullsavename, \"wb\") as f:\n",
    "    pickle.dump(reshape_to_expected_fullinput(training_set_data), f)\n",
    "fullsavename = []\n",
    "with open(facesavename, \"wb\") as f:\n",
    "    pickle.dump(reshape_to_expected_faceinput(training_set_data), f)\n",
    "facesavename = []\n",
    "t2 = datetime.datetime.utcnow()\n",
    "#Measuring execution time\n",
    "print('Elapsed time: ' + str(t2-t1))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set_data = []\n",
    "# path = '/home/ssrlab/qx/Big5/train'\n",
    "# gt = pickle.load( open( \"/home/ssrlab/qx/Big5/gt/annotation_training.pkl\", \"rb\" ), encoding='latin1' )\n",
    "# t1 = datetime.datetime.utcnow()\n",
    "# i=1\n",
    "# continue_start_num=1\n",
    "# partnum = 1\n",
    "# filenum=1 #500개의 비디오마다 하나의 파일에 저장\n",
    "# count=0 #500개의 데이터가 있는지 검증\n",
    "# for filename in os.listdir(path):#파일 내 비디오 둘러보기\n",
    "#     filePath = path+'/'+filename\n",
    "#     alldata=preprocessing_input(file_path= filePath, file_name= filename, dictionary= gt, training= True)\n",
    "#     if partnum < 12:\n",
    "#         if count==500:\n",
    "#             fullsavename = '/home/ssrlab/qx/video-swin-transformer-pytorch/data/32Framesfullandfaceandaudio/full/train_set{}.dat'.format(filenum)\n",
    "#             facesavename = '/home/ssrlab/qx/video-swin-transformer-pytorch/data/32Framesfullandfaceandaudio/face/train_set{}.dat'.format(filenum)\n",
    "#             with open(fullsavename, \"wb\") as f:\n",
    "#                 pickle.dump(reshape_to_expected_fullinput(training_set_data), f)\n",
    "#             with open(facesavename, \"wb\") as f:\n",
    "#                 pickle.dump(reshape_to_expected_faceinput(training_set_data), f)\n",
    "#             count=0\n",
    "#             filenum+=1\n",
    "#             training_set_data = []\n",
    "#             partnum+=1\n",
    "#         else:\n",
    "#             if int(get_number_of_frames(filePath))>128:\n",
    "#                 if alldata[0] is None:\n",
    "#                     continue\n",
    "#                 else:\n",
    "#                     training_set_data.append(alldata)\n",
    "#                     # print(len(reshape_to_expected_input(reshape_to_expected_fullinput(training_set_data))[0][1]))\n",
    "#                     print('6000/',i)\n",
    "#                     i+=1\n",
    "#                     count+=1\n",
    "#     else:\n",
    "#         fullsavename = '/home/ssrlab/qx/video-swin-transformer-pytorch/data/32Framesfullandfaceandaudio/full/train_set{}.dat'.format(filenum)\n",
    "#         facesavename = '/home/ssrlab/qx/video-swin-transformer-pytorch/data/32Framesfullandfaceandaudio/face/train_set{}.dat'.format(filenum)\n",
    "#         if int(get_number_of_frames(filePath))>128:\n",
    "#             if alldata[0] is None:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 training_set_data.append(alldata)\n",
    "#                 print('6000/',i)\n",
    "#                 i+=1\n",
    "#                 count+=1\n",
    "# with open(fullsavename, \"wb\") as f:\n",
    "#     pickle.dump(reshape_to_expected_fullinput(training_set_data), f)\n",
    "# fullsavename = []\n",
    "# with open(facesavename, \"wb\") as f:\n",
    "#     pickle.dump(reshape_to_expected_faceinput(training_set_data), f)\n",
    "# facesavename = []\n",
    "# t2 = datetime.datetime.utcnow()\n",
    "# #Measuring execution time\n",
    "# print('Elapsed time: ' + str(t2-t1))#\n",
    "# #얻은 데이터를 파일에 저장합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set_data = []\n",
    "# path ='/home/ssrlab/qx/Big5/test'\n",
    "# gt = pickle.load( open( \"/home/ssrlab/qx/Big5/gt/annotation_test.pkl\", \"rb\" ), encoding='latin1' )#태그 정보 얻기\n",
    "# t1 = datetime.datetime.utcnow()\n",
    "# i=1\n",
    "# filenum=1 #500개의 비디오마다 하나의 파일에 저장\n",
    "# for filename in os.listdir(path):#파일 내 비디오 둘러보기\n",
    "#     filePath = path+'/'+filename\n",
    "#     if int(get_number_of_frames(filePath))>280:\n",
    "#         print('2000/',i)\n",
    "#         i+=1\n",
    "#         training_set_data.append(preprocessing_input(file_path= filePath, file_name= filename, dictionary= gt, training= True))\n",
    "# savename = '/home/ssrlab/qx/code/test/video-swin-transformer-pytorch/data/face/15Frames/test/qxtest_set{}.dat'.format(filenum)\n",
    "# with open(savename, \"wb\") as f:\n",
    "#     pickle.dump(training_set_data, f)\n",
    "# t2 = datetime.datetime.utcnow()\n",
    "# #Measuring execution time\n",
    "# print('Elapsed time: ' + str(t2-t1))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set_data = []\n",
    "# path ='/home/ssrlab/qx/Big5/train'\n",
    "# gt = pickle.load( open( \"/home/ssrlab/qx/Big5/gt/annotation_training.pkl\", \"rb\" ), encoding='latin1' )#태그 정보 얻기\n",
    "# t1 = datetime.datetime.utcnow()\n",
    "# i=1\n",
    "# filenum=1 #500개의 비디오마다 하나의 파일에 저장\n",
    "# count=0 #500개의 데이터가 있는지 검증\n",
    "# for filename in os.listdir(path):#파일 내 비디오 둘러보기\n",
    "#     filePath = path+'/'+filename\n",
    "#     if count==500:\n",
    "#         savename = '/home/ssrlab/qx/code/test/video-swin-transformer-pytorch/data/face/15Frames/train/qxtrain_set{}.dat'.format(filenum)\n",
    "#         with open(savename, \"wb\") as f:\n",
    "#             pickle.dump(training_set_data, f)\n",
    "#         count=1\n",
    "#         filenum+=1\n",
    "#         training_set_data = []\n",
    "#     else:\n",
    "#         if int(get_number_of_frames(filePath))>280:\n",
    "#             print('6000/',i)\n",
    "#             i+=1\n",
    "#             training_set_data.append(preprocessing_input(file_path= filePath, file_name= filename, dictionary= gt, training= True))\n",
    "#             count+=1\n",
    "# t2 = datetime.datetime.utcnow()\n",
    "# #Measuring execution time\n",
    "# print('Elapsed time: ' + str(t2-t1))#\n",
    "# #얻은 데이터를 파일에 저장합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_set_data = []\n",
    "# path = '/home/ssrlab/qx/Big5/valid'\n",
    "# gt = pickle.load( open( \"/home/ssrlab/qx/Big5/gt/annotation_validation.pkl\", \"rb\" ), encoding='latin1' )\n",
    "# t1 = datetime.datetime.utcnow()\n",
    "# i=1\n",
    "# filenum=1\n",
    "# count=0\n",
    "# for filename in os.listdir(path):#파일 내 비디오 둘러보기\n",
    "#     filePath = path+'/'+filename\n",
    "#     if count==500:\n",
    "#         savename = '/home/ssrlab/qx/code/test/video-swin-transformer-pytorch/data/face/15Frames/valid/qxvalid_set{}.dat'.format(filenum)\n",
    "#         with open(savename, \"wb\") as f:\n",
    "#             pickle.dump(validation_set_data, f)\n",
    "#         count=1\n",
    "#         filenum+=1\n",
    "#         validation_set_data = []\n",
    "#     else:\n",
    "#         if int(get_number_of_frames(filePath))>280:\n",
    "#             print('6000/',i)\n",
    "#             i+=1\n",
    "#             validation_set_data.append(preprocessing_input(file_path= filePath, file_name= filename, dictionary= gt, training= True))\n",
    "#             count+=1\n",
    "# t2 = datetime.datetime.utcnow()\n",
    "# #Measuring execution time\n",
    "# print('Elapsed time: ' + str(t2-t1))#\n",
    "# #얻은 데이터를 파일에 저장합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set_data = []\n",
    "# path = '/home/ssrlab/qx/Big5/test'\n",
    "# gt = pickle.load( open( \"/home/ssrlab/qx/Big5/gt/annotation_test.pkl\", \"rb\" ), encoding='latin1' )\n",
    "# t1 = datetime.datetime.utcnow()\n",
    "# i=1\n",
    "# filenum=1\n",
    "# count=0\n",
    "# for filename in os.listdir(path):#파일 내 비디오 둘러보기\n",
    "#     filePath = path+'/'+filename\n",
    "#     if count==500:\n",
    "#         savename = '/home/ssrlab/qx/code/test/video-swin-transformer-pytorch/data/face/15Frames/test/qxtest_set{}.dat'.format(filenum)\n",
    "#         with open(savename, \"wb\") as f:\n",
    "#             pickle.dump(test_set_data, f)\n",
    "#         count=1\n",
    "#         filenum+=1\n",
    "#         test_set_data = []\n",
    "#     else:\n",
    "#         if int(get_number_of_frames(filePath))>280:\n",
    "#             print('2000/',i)\n",
    "#             i+=1\n",
    "#             test_set_data.append(preprocessing_input(file_path= filePath, file_name= filename, dictionary= gt, training= True))\n",
    "#             count+=1\n",
    "# t2 = datetime.datetime.utcnow()\n",
    "# #Measuring execution time\n",
    "# print('Elapsed time: ' + str(t2-t1))#\n",
    "# #얻은 데이터를 파일에 저장합니다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qx_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
